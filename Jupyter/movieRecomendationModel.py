# -*- coding: utf-8 -*-
"""Movies.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Xv3lD4RC8-MOP9ClykY0P_YezWgwdcX

# Recommender System: Final Proyect ME396P
## Initialize
"""

import numpy as np
import pandas as pd
# import missingno as msno
import matplotlib.pyplot as plt 
import pickle


# ## Load Data


# Load credits dataframe

dfCredits = pd.read_csv('../Datasets/credits.csv')
dfCredits.head()


# Load keywords dataframe

dfKeywords = pd.read_csv('../Datasets/keywords.csv')
dfKeywords.head()


# Load links small dataframe

dfLinks = pd.read_csv('../Datasets/links_small.csv')
dfLinks.head()


# Load movies metadata dataframe

dfMovies = pd.read_csv('../Datasets/movies_metadata.csv')
dfMovies.head()

# Remove unformatted ids

dfMovies = dfMovies[dfMovies['id'].apply(lambda x: str(x).isdigit())]

dfMovies[['id']] = dfMovies[['id']].apply(pd.to_numeric)

# Load ratings reduced dataframe

dfRatings = pd.read_csv('../Datasets/ratings_small.csv')
dfRatings.head()

# We are going to add the credits information to the movies metadata dataframe

dfMovies = dfMovies.merge(dfCredits, on='id')
dfMovies.head()

# We are also going to merge the ids on the ratings dataset

dfRatings = dfRatings.rename(columns={'movieId': 'id'})
dfRatings = dfRatings.merge(dfMovies,on='id')

# Keep only useful columns
dfRatings = dfRatings[['userId', 'id', 'rating', 'original_title']]
dfRatings.head()

"""## Prepare Data"""

# We will be using the overview + tagline column to create a description column

# Visualize missing values as a matrix
# msno.matrix(dfMovies)

# Let's drop columns that are almost empty, and rows that don't have neither description AND tagline

dfMovies = dfMovies.drop(['belongs_to_collection', 'homepage'], axis=1)

dfMovies = dfMovies.dropna(subset=['overview', 'tagline'], how='all')

dfMovies.shape

# Removing duplicates by title, keeping the first appearance
dfMovies.drop_duplicates(subset='title', keep='first', inplace=True)

dfMovies.shape

# Prepare description column

dfMovies['overview'] = dfMovies['overview'].fillna('')
dfMovies['tagline'] = dfMovies['tagline'].fillna('')

dfMovies['description'] = dfMovies['overview'] + ' ' + dfMovies['tagline']

# Print an example of the resulting column
dfMovies['description'][1]

"""## Content Based Filtering"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Remove all english stop words such as 'the', 'a', 'and'
mdlTfvMvs = TfidfVectorizer(stop_words='english')

tfidf_matrix = mdlTfvMvs.fit_transform(dfMovies['description'])

tfidf_matrix.shape

# There are 41379 movies and 75293 different words

# We could use the cosine_similarity() method, but we'll rather use linear_kernel 
# since we are doing a dot product (comparing all movies with all movies)

from sklearn.metrics.pairwise import linear_kernel

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
cosine_sim.shape

# THIS IS AN ALTERNATIVE -> uncomment if desired

# from sklearn.metrics.pairwise import cosine_similarity

# Compute the cosine similarity matrix
# cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix, dense_output=True)
# cosine_sim.shape

# Let us create a title-indexed relation

titles = dfMovies['title']

indices = pd.Series(dfMovies.index,index=dfMovies['title'])
indices

def get_recommendations(title):
    # Retrieve the movie index by title
    idx = indices[title] 
    # Retrieve those movies with similarity to whatever passed
    sim_scores = list(enumerate(cosine_sim[idx]))
    # Sort by score, descending order
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # Keep the top 10 most similar movies
    sim_scores = sim_scores[1:11]
    # Return results in df form
    movie_indices = [i[0] for i in sim_scores]
    return titles.iloc[movie_indices]

# get_recommendations('Toy Story')
# .to_list() for list format

"""
## Collaborative Filtering Recommendation"""

# Scale goes from 0 to 5
dfRatings.describe()

print("The list of existing users is:", set(dfRatings['userId'])) # to use for login!

# !pip install scikit-surprise

from surprise import Dataset
from surprise import Reader
from surprise.model_selection import train_test_split

# specify what the rating scale was
reader = Reader(rating_scale=(0, 5))

# the columns must correspond to user id, item id and ratings
X = Dataset.load_from_df(dfRatings[['userId', 'id', 'rating']], reader)

# sample random trainset and testset
X_train, X_test = train_test_split(X, test_size=.25)

from surprise import SVD
from surprise import accuracy

# define SVD model
mdlSvdMvsRtg = SVD()

# fit SVD model
mdlSvdMvsRtg.fit(X_train)

# predict ratings for testset
test_pred = mdlSvdMvsRtg.test(X_test)

# evaluate SVD model
accuracy.rmse(test_pred) # this means that our data will be at most .89 units from the model

# cross validate

from surprise.model_selection import cross_validate

cross_validate(mdlSvdMvsRtg, X, cv=5)

# Score new data points

movieIdx = 350
pred = mdlSvdMvsRtg.predict(1, movieIdx) 
print("Have user", pred.uid, "watched movie with index", movieIdx, "the predicted score would be", 
      (str(round(pred.est, 2)) + "."), "Was the estimation impossible to do?", pred.details['was_impossible'])

# Create dataframe to hold movies in the ratings dataframe
dfRatUnique = dfRatings.drop_duplicates(subset='id', keep="first")
dfRatUnique["predicted_rating"] = ""
dfRatUnique.head()

# Function to get top 10 movies for a specific user
def get_recommendations_per_user(uid, n=10):
  # Apply prediction algorithm for that specific user rating
  dfRatUnique['predicted_rating'] = dfRatUnique.apply(lambda x: get_ind_score(uid, x['id']), axis=1)
  # Sort by descending value
  dfRatUnique.sort_values('predicted_rating', ascending=False, inplace=True)
  # Return first N elements
  return (dfRatUnique['original_title'].tolist())[:n]

# Function to predict the score of a specific movie from a user
def get_ind_score(uid, movieIdx):
  pred = mdlSvdMvsRtg.predict(uid, movieIdx)
  if pred.details['was_impossible'] == False:
    return round(pred.est,2)
  else:
    return -1

# params: for user 1, give me their top 10 predicted-rated movies
get_recommendations_per_user(1)

# Serialize data

pickle.dump(cosine_sim, open('../Serialized_objects/model.pkl', 'wb'))

# Loading the model 
# model = pickle.load(open('model.pkl', 'rb'))

# print(model)

pickle.dump(dfMovies, open('../Serialized_objects/dfMovies.pkl', 'wb'))

# Loading the model 
# dfMoviesRead = pickle.load(open('dfMovies.pkl', 'rb'))

pickle.dump(dfRatings, open('../Serialized_objects/dfRatings.pkl', 'wb'))

# print(dfMoviesRead)